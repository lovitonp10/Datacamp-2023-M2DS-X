{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Decision Trees to Random Forests\n",
    "\n",
    "```\n",
    "Authors: Alexandre Gramfort\n",
    "         Thomas Moreau\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that by increasing the depth of the tree, we are going to get an over-fitted model. A way to bypass the choice of a specific depth it to combine several trees together.\n",
    "\n",
    "Let's start by training several trees on slightly different data. The slightly different dataset could be generated by randomly sampling with replacement. In statistics, this called a boostrap sample. We will use the iris dataset to create such ensemble and ensure that we have some data for training and some left out data for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before to train several decision trees, we will run a single tree. However, instead to train this tree on `X_train`, we want to train it on a bootstrap sample. You can use the `np.random.choice` function sample with replacement some index. You will need to create a sample_weight vector and pass it to the `fit` method of the `DecisionTreeClassifier`. We provide the `generate_sample_weight` function which will generate the `sample_weight` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_idx(X):\n",
    "    indices = np.random.choice(\n",
    "        np.arange(X.shape[0]), size=X.shape[0], replace=True\n",
    "    )\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 27,  14,   1, 111,  74,   9,  28,  34,  72,  83, 104,  15, 103,\n",
       "        85,  18,   3,  67,   7,  76,  26,  91,  20,  17,  17,  29,  76,\n",
       "        75,  81,  13,  26,   0,  14,   2,  47,  41,  78,  44, 106, 106,\n",
       "        90,  56,  50,   4,  15,  69,  27,  26,   0,  68,  18,   7,  73,\n",
       "        31,  88, 108,  11,  81,  32,  79, 104,  49,  50,  49,  92,  20,\n",
       "        44,  27,  46,  39,  86,   2,  97, 106,   7,  48,  31,  42,  36,\n",
       "       107,  64,  82,  79,  85,   6,  82, 111,  74,  33,  83,  18,  56,\n",
       "       100,  80,   5, 109,   2,  52,  17,  27,  42, 108,  18, 105,  96,\n",
       "        66,  81, 108,  58,  81,  95,  97,   3])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bootstrap_idx(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({59: 1,\n",
       "         93: 3,\n",
       "         36: 1,\n",
       "         110: 1,\n",
       "         84: 3,\n",
       "         32: 1,\n",
       "         46: 1,\n",
       "         78: 2,\n",
       "         11: 2,\n",
       "         86: 3,\n",
       "         6: 2,\n",
       "         79: 3,\n",
       "         16: 1,\n",
       "         90: 1,\n",
       "         64: 1,\n",
       "         83: 1,\n",
       "         105: 3,\n",
       "         10: 2,\n",
       "         3: 2,\n",
       "         65: 3,\n",
       "         108: 1,\n",
       "         70: 2,\n",
       "         89: 1,\n",
       "         73: 2,\n",
       "         53: 1,\n",
       "         100: 3,\n",
       "         24: 1,\n",
       "         94: 1,\n",
       "         56: 2,\n",
       "         52: 3,\n",
       "         74: 2,\n",
       "         34: 1,\n",
       "         28: 3,\n",
       "         106: 1,\n",
       "         25: 2,\n",
       "         50: 1,\n",
       "         15: 1,\n",
       "         58: 2,\n",
       "         104: 1,\n",
       "         101: 2,\n",
       "         85: 2,\n",
       "         109: 1,\n",
       "         12: 1,\n",
       "         102: 1,\n",
       "         95: 2,\n",
       "         2: 2,\n",
       "         87: 1,\n",
       "         37: 1,\n",
       "         0: 2,\n",
       "         8: 3,\n",
       "         44: 1,\n",
       "         69: 1,\n",
       "         92: 2,\n",
       "         5: 1,\n",
       "         42: 2,\n",
       "         63: 3,\n",
       "         40: 1,\n",
       "         72: 3,\n",
       "         38: 1,\n",
       "         39: 1,\n",
       "         111: 1,\n",
       "         91: 1,\n",
       "         43: 1,\n",
       "         80: 1,\n",
       "         1: 1,\n",
       "         75: 1,\n",
       "         67: 1,\n",
       "         18: 1,\n",
       "         20: 1,\n",
       "         13: 1})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(bootstrap_idx(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_sample(X, y):\n",
    "    indices = bootstrap_idx(X)\n",
    "    return X[indices], y[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bootstrap, y_train_bootstrap = bootstrap_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes distribution in the original data: Counter({0: 38, 1: 37, 2: 37})\n",
      "Classes distribution in the bootstrap: Counter({2: 41, 1: 37, 0: 34})\n"
     ]
    }
   ],
   "source": [
    "print(f'Classes distribution in the original data: {Counter(y_train)}')\n",
    "print(f'Classes distribution in the bootstrap: {Counter(y_train_bootstrap)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Call--\n",
      "> \u001b[0;32m/Users/pierreloviton/opt/anaconda3/lib/python3.9/site-packages/IPython/core/displayhook.py\u001b[0m(252)\u001b[0;36m__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    251 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 252 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    253 \u001b[0;31m        \"\"\"Printing with history cache management.\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#import ipdb\n",
    "#ipdb.set_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE: Create a bagging classifier</b>:<br>\n",
    "    <br>\n",
    "    A bagging classifier will train several decision tree classifiers, each of them on a different bootstrap sample.\n",
    "     <ul>\n",
    "      <li>\n",
    "      Create several <code>DecisionTreeClassifier</code> and store them in a Python list;\n",
    "      </li>\n",
    "      <li>\n",
    "      Loop over these trees and <code>fit</code> them by generating a bootstrap sample using <code>bootstrap_sample</code> function;\n",
    "      </li>\n",
    "      <li>\n",
    "      To predict with this ensemble of trees on new data (testing set), you can provide the same set to each tree and call the <code>predict</code> method. Aggregate all predictions in a NumPy array;\n",
    "      </li>\n",
    "      <li>\n",
    "      Once the predictions available, you need to provide a single prediction: you can retain the class which was the most predicted which is called a majority vote;\n",
    "      </li>\n",
    "      <li>\n",
    "      Finally, check the accuracy of your model.\n",
    "      </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "\n",
    "# Number of decision trees in the ensemble\n",
    "n_estimators = 10\n",
    "\n",
    "# Create a list to store the decision trees\n",
    "trees = []\n",
    "\n",
    "# Create the decision trees\n",
    "for i in range(n_estimators):\n",
    "    trees.append(DecisionTreeClassifier())\n",
    "\n",
    "# Bootstrap sample the training data\n",
    "X_train, y_train = resample(X_train, y_train)\n",
    "\n",
    "# Fit the decision trees on the bootstrapped data\n",
    "for tree in trees:\n",
    "    tree.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = []\n",
    "for tree in trees:\n",
    "    predictions.append(tree.predict(X_test))\n",
    "\n",
    "# Convert the list of arrays into a 2D array\n",
    "predictions = np.array(predictions)\n",
    "\n",
    "# Take the majority vote of the predictions\n",
    "final_predictions = []\n",
    "for i in range(len(X_test)):\n",
    "    final_predictions.append(np.argmax(np.bincount(predictions[:,i])))\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = sum(final_predictions == y_test) / len(y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9933333333333333"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from statistics import mode\n",
    "list_ = [DecisionTreeClassifier() for _ in range(3)]\n",
    "preds = []\n",
    "for tree in list_:\n",
    "    tree.fit(*bootstrap_sample(X, y))\n",
    "    preds.append(tree.predict(X))\n",
    "pred_ = np.array([mode(np.stack(preds)[:, i]) for i in range(y.size)])\n",
    "np.mean(pred_==y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'mode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m     pred_array\u001b[38;5;241m.\u001b[39mappend(pred)\n\u001b[1;32m     16\u001b[0m pred_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(pred_array)\n\u001b[0;32m---> 17\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m(pred_array, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mmode[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     18\u001b[0m (y_pred \u001b[38;5;241m==\u001b[39m y_test)\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numpy/__init__.py:311\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtesting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tester\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Tester\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    312\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;18m__name__\u001b[39m, attr))\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'mode'"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from statistics import mode\n",
    "\n",
    "list_tree = [DecisionTreeClassifier() for i in range(10)]\n",
    "\n",
    "pred_array = []\n",
    "for tree in list_tree:\n",
    "    X_train_bootstrap, y_train_bootstrap = bootstrap_sample(X_train, y_train)\n",
    "    idx_features = np.random.choice(X.shape[1], size=int(np.sqrt(X.shape[1])), replace=False)\n",
    "    X_train_bootstrap = X_train_bootstrap[:, idx_features]\n",
    "    tree.fit(X_train_bootstrap, y_train_bootstrap)\n",
    "\n",
    "    pred = tree.predict(X_test[:, idx_features])\n",
    "    pred_array.append(pred)\n",
    "\n",
    "pred_array = np.asarray(pred_array)\n",
    "y_pred = np.mode(pred_array, axis=0).mode[0]\n",
    "(y_pred == y_test).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE: using scikit-learn</b>:\n",
    "    <br>\n",
    "    After implementing your own bagging classifier, use a <code>BaggingClassifier</code> from scikit-learn to fit the above data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Number of decision trees in the ensemble\n",
    "n_estimators = 10\n",
    "\n",
    "# Create the bagging classifier\n",
    "bagging_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=n_estimators, bootstrap=True)\n",
    "\n",
    "# Fit the classifier on the training data\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = bagging_clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = sum(y_pred == y_test) / len(y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very famous classifier is the random forest classifier. It is similar to the bagging classifier. In addition of the bootstrap, the random forest will use a subset of features (selected randomly) to find the best split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE: Create a random forest classifier</b>:\n",
    "    <br>\n",
    "    Use your previous code which was generated several <code>DecisionTreeClassifier</code>. Check the list of the option of this classifier and modify one of the parameters such that only the $\\sqrt{F}$ features are used for the splitting. $F$ represents the number of features in the dataset.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Number of decision trees in the ensemble\n",
    "n_estimators = 10\n",
    "\n",
    "# Number of features in the dataset\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "# The number of features to consider when looking for the best split\n",
    "max_features = int(sqrt(n_features))\n",
    "\n",
    "# Create the random forest classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=n_estimators, max_features=max_features)\n",
    "\n",
    "# Fit the classifier on the training data\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = sum(y_pred == y_test) / len(y_test)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE: using scikit-learn</b>:\n",
    "    <br>\n",
    "    After implementing your own random forest classifier, use a <code>RandomForestClassifier</code> from scikit-learn to fit the above data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52a28966291f44688d8768e8fc2674f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='max_depth', max=8), Output()), _dom_classes=('widget-int…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from figures import plot_forest_interactive\n",
    "plot_forest_interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
